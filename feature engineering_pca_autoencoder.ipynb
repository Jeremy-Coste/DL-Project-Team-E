{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T12:44:09.131674Z",
     "start_time": "2018-10-02T12:44:09.129175Z"
    }
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:38:03.460211Z",
     "start_time": "2018-10-03T22:37:49.443514Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import argparse\n",
    "# Needed for PCA\n",
    "from sklearn import decomposition\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderbook_cols = ['{}_{}_{}'.format(s,t,l) for l in range(1,6) for s in ['ask','bid'] for t in ['price', 'vol'] ]\n",
    "orderbook_ori = pd.read_csv('Intel_orderbook.csv', \\\n",
    "                            header = None, names = orderbook_cols)\n",
    "orderbook1 = orderbook_ori.copy()\n",
    "orderbook1['mid_price'] = (orderbook1.iloc[:,0] + orderbook1.iloc[:,2]) / 2\n",
    "orderbook1['mid_price_mov'] = np.sign(orderbook1['mid_price'].shift(-1)-orderbook1['mid_price'])\n",
    "orderbook2 = orderbook1.dropna()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_all_array = scaler.fit_transform(orderbook2.iloc[:,:len(orderbook_cols)])\n",
    "orderbook = orderbook2.copy()\n",
    "orderbook.iloc[:,:len(orderbook_cols)] = x_all_array\n",
    "\n",
    "train_weight = 0.8\n",
    "cv_weight = 0.1\n",
    "split1 = int(orderbook.shape[0] * train_weight)\n",
    "split2 = int(orderbook.shape[0] * cv_weight)\n",
    "df_train = orderbook[:split1]\n",
    "df_cv = orderbook[split1:split1+split2]\n",
    "df_test = orderbook[split1+split2:]\n",
    "x_train = df_train.iloc[:,:len(orderbook_cols)]\n",
    "x_train_array = np.array(x_train)\n",
    "#y_train = df_train.iloc[:,-1]\n",
    "x_cv = df_cv.iloc[:,:len(orderbook_cols)]\n",
    "x_cv_array = np.array(x_cv)\n",
    "#y_cv = df_cv.iloc[:,-1]\n",
    "x_test = df_test.iloc[:,:len(orderbook_cols)]\n",
    "x_test_array = np.array(x_test)\n",
    "#y_test = df_test.iloc[:,-1]\n",
    "x_all = orderbook.iloc[:,:len(orderbook_cols)]\n",
    "x_all_array = np.array(x_all)\n",
    "#y_all = orderbook.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:38:03.476437Z",
     "start_time": "2018-10-03T22:38:03.462522Z"
    }
   },
   "outputs": [],
   "source": [
    "input_size = 20\n",
    "#re-constructed size\n",
    "output_size = 20\n",
    "\n",
    "# 3 hidden layers for encoder\n",
    "n_encoder_h_1 = 16\n",
    "n_encoder_h_2 = 8\n",
    "n_encoder_h_3 = 4\n",
    "\n",
    "# 3 hidden layers for decoder\n",
    "n_decoder_h_1 = 4\n",
    "n_decoder_h_2 = 8\n",
    "n_decoder_h_3 = 16\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 20 #200\n",
    "batch_size = 200\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:38:03.527109Z",
     "start_time": "2018-10-03T22:38:03.479053Z"
    }
   },
   "outputs": [],
   "source": [
    "def layer_batch_normalization(x, n_out, phase_train):\n",
    "    \"\"\"\n",
    "    Defines the network layers\n",
    "    input:\n",
    "        - x: input vector of the layer\n",
    "        - n_out: integer, depth of input maps - number of sample in the batch \n",
    "        - phase_train: boolean tf.Varialbe, true indicates training phase\n",
    "    output:\n",
    "        - batch-normalized maps   \n",
    "    \"\"\"\n",
    "\n",
    "    beta_init = tf.constant_initializer(value=0.0, dtype=tf.float32)\n",
    "    beta = tf.get_variable(\"beta\", [n_out], initializer=beta_init)\n",
    "    \n",
    "    gamma_init = tf.constant_initializer(value=1.0, dtype=tf.float32)\n",
    "    gamma = tf.get_variable(\"gamma\", [n_out], initializer=gamma_init)\n",
    "\n",
    "    #tf.nn.moment: https://www.tensorflow.org/api_docs/python/tf/nn/moments\n",
    "    #calculate mean and variance of x\n",
    "    batch_mean, batch_var = tf.nn.moments(x, [0], name='moments')\n",
    "\n",
    "    #tf.train.ExponentialMovingAverage:\n",
    "    #https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n",
    "    #Maintains moving averages of variables by employing an exponential decay.\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.9)\n",
    "    ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "    ema_mean, ema_var = ema.average(batch_mean), ema.average(batch_var)\n",
    "    \n",
    "    def mean_var_with_update():\n",
    "        with tf.control_dependencies([ema_apply_op]):\n",
    "            return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "       \n",
    "    #tf.cond: https://www.tensorflow.org/api_docs/python/tf/cond\n",
    "    #Return true_fn() if the predicate pred is true else false_fn()\n",
    "    mean, var = tf.cond(phase_train, mean_var_with_update, lambda: (ema_mean, ema_var))\n",
    "\n",
    "    reshaped_x = tf.reshape(x, [-1, 1, 1, n_out])\n",
    "    normed = tf.nn.batch_norm_with_global_normalization(reshaped_x, mean, var, beta, gamma, 1e-3, True)\n",
    "    #normed = tf.nn.batch_normalization(reshaped_x, mean, var, beta, gamma, 1e-3, True)\n",
    "    \n",
    "    return tf.reshape(normed, [-1, n_out])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:38:03.574224Z",
     "start_time": "2018-10-03T22:38:03.529227Z"
    }
   },
   "outputs": [],
   "source": [
    "def layer(x, weight_shape, bias_shape, phase_train):\n",
    "    \n",
    "    \"\"\"\n",
    "    Defines the network layers\n",
    "    input:\n",
    "        - x: input vector of the layer\n",
    "        - weight_shape: shape the the weight maxtrix\n",
    "        - bias_shape: shape of the bias vector\n",
    "        - phase_train: boolean tf.Varialbe, true indicates training phase\n",
    "    output:\n",
    "        - output vector of the layer after the matrix multiplication and non linear transformation\n",
    "    \"\"\"\n",
    "    \n",
    "    #initialize weights\n",
    "    weight_init = tf.random_normal_initializer(stddev=(1.0/weight_shape[0])**0.5)\n",
    "    W = tf.get_variable(\"W\", weight_shape, initializer=weight_init)\n",
    "    \n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    b = tf.get_variable(\"b\", bias_shape, initializer=bias_init)\n",
    "\n",
    "    logits = tf.matmul(x, W) + b\n",
    "    \n",
    "    #apply the non-linear function after the batch normalization\n",
    "    return tf.nn.sigmoid(layer_batch_normalization(logits, weight_shape[1], phase_train))\n",
    "    # Using sigmoid to avoid sharp transitions in neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T13:39:04.039484Z",
     "start_time": "2018-10-02T13:39:04.036698Z"
    }
   },
   "source": [
    "# Definition of the Encoder Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:38:03.598508Z",
     "start_time": "2018-10-03T22:38:03.576953Z"
    }
   },
   "outputs": [],
   "source": [
    "def encoder(x, n_code, phase_train):\n",
    "    \"\"\"\n",
    "    Defines the network encoder part\n",
    "    input:\n",
    "        - x: input vector of the encoder\n",
    "        - n_code: number of neurons in the code layer (output of the encoder - input of the decoder) \n",
    "        - phase_train: boolean tf.Varialbe, true indicates training phase\n",
    "    output:\n",
    "        - output vector: reduced dimension\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope(\"encoder\"):\n",
    "        \n",
    "        with tf.variable_scope(\"h_1\"):\n",
    "            h_1 = layer(x, [input_size, n_encoder_h_1], [n_encoder_h_1], phase_train)\n",
    "\n",
    "        with tf.variable_scope(\"h_2\"):\n",
    "            h_2 = layer(h_1, [n_encoder_h_1, n_encoder_h_2], [n_encoder_h_2], phase_train)\n",
    "\n",
    "        with tf.variable_scope(\"h_3\"):\n",
    "            h_3 = layer(h_2, [n_encoder_h_2, n_encoder_h_3], [n_encoder_h_3], phase_train)\n",
    "\n",
    "        with tf.variable_scope(\"code\"):\n",
    "            output = layer(h_3, [n_encoder_h_3, n_code], [n_code], phase_train)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T13:39:56.962874Z",
     "start_time": "2018-10-02T13:39:56.960040Z"
    }
   },
   "source": [
    "# Definition of the Decoder Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:38:03.634567Z",
     "start_time": "2018-10-03T22:38:03.600519Z"
    }
   },
   "outputs": [],
   "source": [
    "def decoder(x, n_code, phase_train):\n",
    "    \"\"\"\n",
    "    Defines the network encoder part\n",
    "    input:\n",
    "        - x: input vector of the decoder - reduced dimension vector\n",
    "        - n_code: number of neurons in the code layer (output of the encoder - input of the decoder)\n",
    "        - phase_train: boolean tf.Varialbe, true indicates training phase\n",
    "    output:\n",
    "        - output vector: reconstructed dimension of the initial vector\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope(\"decoder\"):\n",
    "        \n",
    "        with tf.variable_scope(\"h_1\"):\n",
    "            h_1 = layer(x, [n_code, n_decoder_h_1], [n_decoder_h_1], phase_train)\n",
    "\n",
    "        with tf.variable_scope(\"h_2\"):\n",
    "            h_2 = layer(h_1, [n_decoder_h_1, n_decoder_h_2], [n_decoder_h_2], phase_train)\n",
    "\n",
    "        with tf.variable_scope(\"h_3\"):\n",
    "            h_3 = layer(h_2, [n_decoder_h_2, n_decoder_h_3], [n_decoder_h_3], phase_train)\n",
    "\n",
    "        with tf.variable_scope(\"output\"):\n",
    "            output = layer(h_3, [n_decoder_h_3, output_size], [output_size], phase_train)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T13:40:11.932837Z",
     "start_time": "2018-10-02T13:40:11.929439Z"
    }
   },
   "source": [
    "# Definition of the Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:38:03.646821Z",
     "start_time": "2018-10-03T22:38:03.637606Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss is L2 measure\n",
    "def loss(output, x):\n",
    "    \"\"\"\n",
    "    Compute the loss of the auto-encoder\n",
    "    \n",
    "    intput:\n",
    "        - output: the output of the decoder\n",
    "        - x: true value of the sample batch - this is the input of the encoder\n",
    "        \n",
    "        the two have the same shape (batch_size * num_of_classes)\n",
    "    output:\n",
    "        - loss: loss of the corresponding batch (scalar tensor)\n",
    "    \n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"training\"):\n",
    "        \n",
    "        l2_measure = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(output, x)), 1))\n",
    "        train_loss = tf.reduce_mean(l2_measure)\n",
    "        train_summary_op = tf.summary.scalar(\"train_cost\", train_loss)\n",
    "        return train_loss, train_summary_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:38:03.661733Z",
     "start_time": "2018-10-03T22:38:03.650651Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using Adam as optimizer for training \t\n",
    "def training(cost, global_step):\n",
    "    \"\"\"\n",
    "    defines the necessary elements to train the network\n",
    "    \n",
    "    intput:\n",
    "        - cost: the cost is the loss of the corresponding batch\n",
    "        - global_step: number of batch seen so far, it is incremented by one \n",
    "        each time the .minimize() function is called\n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:38:03.705246Z",
     "start_time": "2018-10-03T22:38:03.664581Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(output, x):\n",
    "    \"\"\"\n",
    "    evaluates the accuracy on the validation set \n",
    "    input:\n",
    "        - output: prediction vector of the network for the validation set\n",
    "        - x: true value for the validation set\n",
    "    output:\n",
    "        - val_loss: loss of the autoencoder\n",
    "        - val_summary_op: summary of the loss\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope(\"validation\"):\n",
    "        \n",
    "        l2_norm = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(output, x, name=\"val_diff\")), 1))\n",
    "        \n",
    "        val_loss = tf.reduce_mean(l2_norm)\n",
    "        \n",
    "        val_summary_op = tf.summary.scalar(\"val_cost\", val_loss)\n",
    "        \n",
    "        return val_loss, val_summary_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T13:51:07.293398Z",
     "start_time": "2018-10-02T13:51:07.282274Z"
    }
   },
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:48:13.428734Z",
     "start_time": "2018-10-03T22:38:03.731572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 3.749076681\n",
      "Validation Loss: 5.40119\n",
      "Model saved in file: /Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master/logs\n",
      "Epoch: 0002 cost = 3.494601849\n",
      "Validation Loss: 5.028652\n",
      "Model saved in file: /Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master/logs\n",
      "Epoch: 0003 cost = 3.378989137\n",
      "Validation Loss: 4.9648376\n",
      "Model saved in file: /Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master/logs\n",
      "Epoch: 0004 cost = 3.276433598\n",
      "Validation Loss: 5.4777546\n",
      "Model saved in file: /Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master/logs\n",
      "Epoch: 0005 cost = 3.266412128\n",
      "Validation Loss: 5.67955\n",
      "Model saved in file: /Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master/logs\n",
      "Epoch: 0006 cost = 3.272652998\n",
      "Validation Loss: 6.213703\n",
      "Model saved in file: /Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master/logs\n",
      "Epoch: 0007 cost = 3.251259510\n",
      "Validation Loss: 6.018215\n",
      "Model saved in file: /Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master/logs\n",
      "Epoch: 0008 cost = 3.214901133\n",
      "Validation Loss: 5.7282915\n",
      "Model saved in file: /Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master/logs\n",
      "Epoch: 0009 cost = 3.209409362\n",
      "Validation Loss: 6.2167606\n",
      "Model saved in file: /Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master/logs\n",
      "Epoch: 0010 cost = 3.160827033\n",
      "Validation Loss: 6.8818226\n",
      "Model saved in file: /Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master/logs\n",
      "Epoch: 0011 cost = 3.120349450\n",
      "Validation Loss: 6.487204\n",
      "Model saved in file: /Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master/logs\n",
      "Epoch: 0012 cost = 3.148564984\n",
      "Validation Loss: 6.844283\n",
      "Model saved in file: /Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master/logs\n",
      "Epoch: 0013 cost = 3.089280939\n",
      "Validation Loss: 4.621412\n",
      "Model saved in file: /Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master/logs\n",
      "Epoch: 0014 cost = 3.081996503\n",
      "Validation Loss: 4.927048\n",
      "Model saved in file: /Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master/logs\n",
      "Epoch: 0015 cost = 3.048305095\n",
      "Validation Loss: 7.4424314\n",
      "Model saved in file: /Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master/logs\n",
      "Epoch: 0016 cost = 3.029832292\n",
      "Validation Loss: 5.9878254\n",
      "Model saved in file: /Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master/logs\n",
      "Epoch: 0017 cost = 3.028874645\n",
      "Validation Loss: 5.1889696\n",
      "Model saved in file: /Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master/logs\n",
      "Epoch: 0018 cost = 3.052118879\n",
      "Validation Loss: 7.0623684\n",
      "Model saved in file: /Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master/logs\n",
      "Epoch: 0019 cost = 3.135064791\n",
      "Validation Loss: 7.437358\n",
      "Model saved in file: /Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master/logs\n",
      "Epoch: 0020 cost = 3.054418958\n",
      "Validation Loss: 5.204483\n",
      "Model saved in file: /Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master/logs\n",
      "Optimization Done\n",
      "Test Loss: 6.978288\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    #parser = argparse.ArgumentParser(description='Autoencoder')\n",
    "    #parser.add_argument('n_code', nargs=1, type=str)\n",
    "    #args = parser.parse_args(['--help'])\n",
    "    #n_code = args.n_code[0]\n",
    "    \n",
    "    #if a jupyter file, please comment the 4 above and use the one bellow\n",
    "    n_code = '2'\n",
    "    \n",
    "    #feel free to change with your own\n",
    "    model_path = '/Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master/logs'\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        with tf.variable_scope(\"autoencoder_model\"):\n",
    "\n",
    "\n",
    "            #the input variables are first define as placeholder \n",
    "            # a placeholder is a variable/data which will be assigned later \n",
    "            x = tf.placeholder(\"float\", [None, 20]) # 20 original features\n",
    "            \n",
    "            phase_train = tf.placeholder(tf.bool)\n",
    "\n",
    "            #define the encoder \n",
    "            code = encoder(x, int(n_code), phase_train)\n",
    "\n",
    "            #define the decoder\n",
    "            output = decoder(code, int(n_code), phase_train)\n",
    "\n",
    "            #compute the loss \n",
    "            cost, train_summary_op = loss(output, x)\n",
    "\n",
    "            #initialize the value of the global_step variable \n",
    "            # recall: it is incremented by one each time the .minimise() is called\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "            train_op = training(cost, global_step)\n",
    "\n",
    "            #evaluate the accuracy of the network (done on a validation set)\n",
    "            eval_op, val_summary_op = evaluate(output, x)\n",
    "\n",
    "            summary_op = tf.summary.merge_all()\n",
    "\n",
    "            #save and restore variables to and from checkpoints.\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            #defines a session\n",
    "            sess = tf.Session()\n",
    "\n",
    "            # summary writer\n",
    "            #https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter\n",
    "            train_writer = tf.summary.FileWriter(model_path, graph=sess.graph)\n",
    "            val_writer   = tf.summary.FileWriter(model_path, graph=sess.graph)\n",
    "\n",
    "            #initialization of the variables\n",
    "            init_op = tf.global_variables_initializer()\n",
    "\n",
    "            sess.run(init_op)\n",
    "\n",
    "            # Training cycle\n",
    "            for epoch in range(training_epochs):\n",
    "\n",
    "                avg_cost = 0.\n",
    "                total_batch = int(x_train_array.shape[0]/batch_size)\n",
    "                \n",
    "                #train_writer = tf.summary.FileWriter(model_path+str(epoch)+'/model.ckpt', graph=sess.graph)\n",
    "                #val_writer   = tf.summary.FileWriter(model_path+str(epoch)+'/model.ckpt', graph=sess.graph)\n",
    "                \n",
    "                # Loop over all batches\n",
    "                for i in range(total_batch):\n",
    "                    \n",
    "                    minibatch_x = x_train_array[i*batch_size:(i+1)*batch_size]\n",
    "                    \n",
    "                    # Fit training using batch data\n",
    "                    #the training is done using the training dataset\n",
    "                    _, new_cost, train_summary = sess.run([train_op, cost, train_summary_op], feed_dict={x: minibatch_x, phase_train: True})\n",
    "                    \n",
    "                    train_writer.add_summary(train_summary, sess.run(global_step))\n",
    "                    \n",
    "                    # Compute average loss\n",
    "                    avg_cost += new_cost/total_batch\n",
    "                \n",
    "                # Display logs per epoch step\n",
    "                if epoch % display_step == 0:\n",
    "                    \n",
    "                    print(\"Epoch:\", '%04d' % (epoch+1), \"cost =\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "                    train_writer.add_summary(train_summary, sess.run(global_step))\n",
    "\n",
    "                    validation_loss, val_summary = sess.run([eval_op, val_summary_op], feed_dict={x: x_cv_array, phase_train: False})\n",
    "                    \n",
    "                    val_writer.add_summary(val_summary, sess.run(global_step))\n",
    "                    \n",
    "                    print(\"Validation Loss:\", validation_loss)\n",
    "\n",
    "                    save_path = saver.save(sess, model_path)\n",
    "                    print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "\n",
    "            print(\"Optimization Done\")\n",
    "\n",
    "            test_loss = sess.run(eval_op, feed_dict={x: x_test_array, phase_train: False})\n",
    "\n",
    "            print(\"Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing PCA\n",
      "PCA Codes\n",
      "[[  6.83314187  -2.69809518]\n",
      " [  6.83204746  -2.69698052]\n",
      " [  6.81168241  -2.70884073]\n",
      " ...\n",
      " [-15.09461315  15.5592874 ]\n",
      " [-15.07970768  15.53025868]\n",
      " [-15.08080209  15.53137334]]\n",
      "\n",
      "\n",
      "Starting Autoencoder /Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master/logs\n",
      "\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master/logs\n",
      "Model restored from file: None\n",
      "Running Autoencoder & Autoencoder Codes\n",
      "\n",
      "\n",
      "[[0.9395733  0.9627585 ]\n",
      " [0.9396074  0.96276367]\n",
      " [0.9395216  0.96275073]\n",
      " ...\n",
      " [0.90304375 0.95878714]\n",
      " [0.9029956  0.95878166]\n",
      " [0.9029959  0.95878154]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    #feel free to change with your own\n",
    "    args_savepath = '/Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master/logs'\n",
    "    new_features_resultpath = '/Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master'\n",
    "    n_code = 2\n",
    "    \n",
    "    #=====================================\n",
    "    # PCA\n",
    "    print ('Performing PCA')\n",
    "    pca = decomposition.PCA(n_components=2) # grid search for the parameter\n",
    "    pca.fit(x_train_array) # use train data for feature selection in order to avoid look ahead bias\n",
    "    print('PCA Codes')\n",
    "    pca_codes = pca.transform(x_all_array)\n",
    "    print(pca_codes)\n",
    "    pca_codes_df = pd.DataFrame(pca_codes)\n",
    "    ob_new_pca = pd.concat([pca_codes_df,orderbook.iloc[:,-1]],axis = 1)\n",
    "    filename = new_features_resultpath + 'ob_new_pca.csv'\n",
    "    ob_new_pca.to_csv(filename)\n",
    "    \n",
    "    '''\n",
    "    print('Re-Constructing')\n",
    "    # transform data into its original space\n",
    "    pca_reconstructed = pca.inverse_transform(pca_codes[:20])\n",
    "    #print(pca_reconstructed)\n",
    "    '''\n",
    "\n",
    "    #=====================================\n",
    "    # AutoEncoder\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        with tf.variable_scope(\"autoencoder_model\"):\n",
    "\n",
    "            x = tf.placeholder(\"float\", [None, 20]) # 20 original features\n",
    "            \n",
    "            phase_train = tf.placeholder(tf.bool)\n",
    "\n",
    "            code = encoder(x, n_code, phase_train)\n",
    "\n",
    "            output = decoder(code, n_code, phase_train)\n",
    "\n",
    "            cost, train_summary_op = loss(output, x)\n",
    "\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "            train_op = training(cost, global_step)\n",
    "\n",
    "            eval_op, val_summary_op = evaluate(output, x)\n",
    "\n",
    "            #saver = tf.train.Saver()\n",
    "            #sess = tf.Session()\n",
    "            print('\\n')\n",
    "            print('Starting Autoencoder', args_savepath ) #args.savepath[0]\n",
    "            print('\\n')\n",
    "            \n",
    "            sess = tf.Session()\n",
    "            saver = tf.train.Saver()\n",
    "            save_path = saver.restore(sess, args_savepath ) #args.savepath[0]\n",
    "            print(\"Model restored from file: %s\" % save_path)\n",
    "\n",
    "            print('Running Autoencoder & Autoencoder Codes')\n",
    "            print('\\n')\n",
    "            \n",
    "            ae_codes = sess.run(code, feed_dict={x: x_all_array, phase_train: True})\n",
    "            print(ae_codes)\n",
    "            ae_codes_df = pd.DataFrame(ae_codes)\n",
    "            ob_new_ae = pd.concat([ae_codes_df,orderbook.iloc[:,-1]],axis = 1)\n",
    "            filename = new_features_resultpath + 'ob_new_ae.csv'\n",
    "            ob_new_ae.to_csv(filename)\n",
    "            \n",
    "            #ae_codes, ae_reconstruction = sess.run([code, output], feed_dict={x: mnist.test.images*np.random.randint(2, size=(784)), phase_train: True})\n",
    "            ob_new_ae.to_csv('feature_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: meihuaren\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#-- import data\n",
    "orderbook_cols = ['{}_{}_{}'.format(s,t,l) for l in range(1,6) for s in ['ask','bid'] for t in ['price', 'vol'] ]\n",
    "orderbook_ori = pd.read_csv('Intel_orderbook.csv', \\\n",
    "                            header = None, names = orderbook_cols)\n",
    "orderbook = orderbook_ori.copy()\n",
    "orderbook['mid_price'] = (orderbook.iloc[:,0] + orderbook.iloc[:,2]) / 2\n",
    "orderbook['mid_price_mov'] = np.sign(orderbook['mid_price'].shift(-1)-orderbook['mid_price']) # the last one is nan\n",
    "\n",
    "message_cols = ['time', 'type', 'order ID', 'size', 'price', 'direction']\n",
    "message = pd.read_csv('Intel_message.csv', \\\n",
    "                          header = None, names = message_cols)\n",
    "\n",
    "total_data = pd.concat([message, orderbook], axis = 1)\n",
    "\n",
    "#-- new feature 1: order flow (could try different lagged periods)\n",
    "'''\n",
    "order flow = the ratio of the volume of market buy(sell) orders arriving in the prior 50 observations \n",
    "             to the resting volume of ask(bid) limit orders at the top of book\n",
    "This feature is constructed according to the paper.\n",
    "# Since we do not have \"number of order\" data, here just use \"volume of order\" instead.\n",
    "Intuition: an increase in this ratio will more likely deplete the best ask level and the mid-price will up-tick,\n",
    "           and vice-versa for a down-tick.\n",
    "'''\n",
    "total_data['type_direction'] = total_data['type'] * total_data['direction']\n",
    "total_data['buy_vol'] = 0\n",
    "buy_order_index1 = total_data[total_data['type_direction'] == -4].index\n",
    "total_data.loc[buy_order_index1, 'buy_vol'] = total_data['size']\n",
    "# Not include type=5(Execution of a hidden limit order), since its update does not change the limit order book.\n",
    "#buy_order_index2 = total_data[total_data['type_direction'] == -5].index\n",
    "#total_data.loc[buy_order_index2, 'buy_vol'] = total_data['size']\n",
    "total_data['sell_vol'] = 0\n",
    "sell_order_index1 = total_data[total_data['type_direction'] == 4].index\n",
    "total_data.loc[sell_order_index1, 'sell_vol'] = total_data['size']\n",
    "#sell_order_index2 = total_data[total_data['type_direction'] == 5].index\n",
    "#total_data.loc[sell_order_index2, 'sell_vol'] = total_data['size']\n",
    "\n",
    "total_data['order_flow_buy'] = total_data['buy_vol'].rolling(50, min_periods = 1).sum() / total_data['ask_vol_1']\n",
    "total_data['order_flow_sell'] = total_data['sell_vol'].rolling(50, min_periods = 1).sum() / total_data['bid_vol_1']\n",
    "\n",
    "#-- new feature 2: liquidity imbalance\n",
    "'''\n",
    "liquidity imbalance at level i = ask_vol_i / (ask_vol_i + bid_vol_i)\n",
    "This feature is constructed according to the ppt.\n",
    "'''\n",
    "for i in range(1,6):\n",
    "    total_data['liq_imb_'+str(i)] = total_data['ask_vol_'+str(i)] \\\n",
    "                                  / (total_data['ask_vol_'+str(i)] + total_data['bid_vol_'+str(i)])\n",
    "\n",
    "#-- new feature 3: actual spread\n",
    "'''\n",
    "actual spread = ask_price_1 - bid_price_1\n",
    "This feature is constructed according to:\n",
    "1. Michael Kearns..._Machine Learning for Market Microstructure...P7\n",
    "2. Irene Aldridge_High-frequency trading...(2013) P190:\n",
    "   First suggested by Bagehot (1971) and later developed by numerous researchers, the bid-ask spread \n",
    "   reflects the expectations of market movements by the market maker using asymmetric information.\n",
    "'''\n",
    "total_data['actual_spread'] = total_data['ask_price_1'] - total_data['bid_price_1']\n",
    "\n",
    "#-- new feature 4: actual market imbalance (could try different lagged periods)\n",
    "'''\n",
    "actual market imbalance = the volume of market buy orders arriving in the prior 50 observations\n",
    "                        - the volume of market sell orders arriving in the prior 50 observations\n",
    "This feature is derived from paper: Michael Kearns..._Machine Learning for Market Microstructure...P8\n",
    "'''\n",
    "total_data['actual_mkt_imb'] = total_data['buy_vol'].rolling(50, min_periods = 1).sum()\\\n",
    "                             - total_data['sell_vol'].rolling(50, min_periods = 1).sum()\n",
    "\n",
    "#-- new feature 5: relative market imbalance (could try different lagged periods)\n",
    "'''\n",
    "relative market imbalance = actual market imbalance / actual spread\n",
    "This feature is derived from paper: Michael Kearns..._Machine Learning for Market Microstructure...P8\n",
    "Intuition: a small actual spread combined with a strongly positive actual market imbalance\n",
    "           would indicate buying pressure.\n",
    "'''\n",
    "total_data['relative_mkt_imb'] = total_data['actual_mkt_imb'] / total_data['actual_spread']\n",
    "\n",
    "#-- new feature 6: relative_mid_price_trend\n",
    "'''\n",
    "First, construct a variation on mid-price where the average of the bid and ask prices is weighted \n",
    "according to their inverse volume. Then, divide this variation by common mid price.\n",
    "This feature is derived from paper: Michael Kearns..._Machine Learning for Market Microstructure...P10\n",
    "Intuition: a larger relative_mid_price_trend would more likely lead to a up-tick.\n",
    "'''\n",
    "\n",
    "total_data['mid_price_inv_vol_weighted'] = (total_data['ask_price_1'] / total_data['ask_vol_1'] \\\n",
    "                                         + total_data['bid_price_1'] / total_data['bid_vol_1'])\\\n",
    "                                         / (1 / total_data['ask_vol_1'] + 1 / total_data['bid_vol_1'])\n",
    "total_data['relative_mid_price_trend'] = total_data['mid_price_inv_vol_weighted'] / total_data['mid_price']\n",
    "\n",
    "#-- new feature 7: relative spread\n",
    "'''\n",
    "relative spread =  (actual spread / mid price) * 10000\n",
    "This feature is derived from paper: Angelo Ranaldo..._Order aggressiveness in limit order book markets...P4\n",
    "'''\n",
    "total_data['relative_spread'] = (total_data['actual_spread'] / total_data['mid_price']) * 1000\n",
    "\n",
    "#-- new feature 8: volatility (could try different lagged periods)\n",
    "'''\n",
    "The volatility is the standard deviation of the last 50 midquote returns then divided by 100\n",
    "This feature is derived from paper: Angelo Ranaldo..._Order aggressiveness in limit order book markets...P4\n",
    "'''\n",
    "total_data['mid_price_return'] = total_data['mid_price'].shift(-1) - total_data['mid_price']\n",
    "total_data['volatility'] = (total_data['mid_price_return'].rolling(50, min_periods = 1).std()) / 100\n",
    "\n",
    "#-- new feature 9: limit order aggressiveness (could try different lagged periods)\n",
    "'''\n",
    "bid(ask) limit order aggressiveness = the ratio of bid(ask) limit orders submitted at no lower(higher) than \n",
    "                                                   the best bid(ask) prices in the prior 50 observations\n",
    "                                                to total bid(ask) limit orders submitted in prior 50 observations\n",
    "This feature is derived from book: Irene Aldridge_High-frequency trading...(2013) P186\n",
    "Intuition: The higher the ratio, the more aggressive is the trader in his bid(ask) to capture the best \n",
    "           available price and the more likely the trader is to believe that the price is about to \n",
    "           move away from the mid price.\n",
    "'''\n",
    "# ask limit order aggressiveness\n",
    "if_ask_sbmt_agr_mid1 = (total_data['type_direction'] == -1)\n",
    "if_ask_sbmt_agr_mid2 = (total_data['price'] <= total_data['ask_price_1'].shift(1))\n",
    "if_ask_sbmt_agr = (if_ask_sbmt_agr_mid1 & if_ask_sbmt_agr_mid2)\n",
    "\n",
    "total_data['if_ask_sbmt_agr'] = if_ask_sbmt_agr\n",
    "if_ask_sbmt_agr_index = total_data[total_data['if_ask_sbmt_agr'] == True].index\n",
    "total_data['ask_vol_sbmt_agr'] = 0\n",
    "total_data.loc[if_ask_sbmt_agr_index, 'ask_vol_sbmt_agr'] = total_data['size']\n",
    "\n",
    "if_ask_sbmt_index = total_data[total_data['type_direction'] == -1].index\n",
    "total_data['ask_vol_sbmt'] = 0\n",
    "total_data.loc[if_ask_sbmt_index, 'ask_vol_sbmt'] = total_data['size']\n",
    "\n",
    "total_data['lo_agr_ask'] = total_data['ask_vol_sbmt_agr'].rolling(50, min_periods = 1).sum()\\\n",
    "                         / total_data['ask_vol_sbmt'].rolling(50, min_periods = 1).sum()\n",
    "\n",
    "# bid limit order aggressiveness\n",
    "if_bid_sbmt_agr_mid1 = (total_data['type_direction'] == 1)\n",
    "if_bid_sbmt_agr_mid2 = (total_data['price'] >= total_data['bid_price_1'].shift(1))\n",
    "if_bid_sbmt_agr = (if_bid_sbmt_agr_mid1 & if_bid_sbmt_agr_mid2)\n",
    "\n",
    "total_data['if_bid_sbmt_agr'] = if_bid_sbmt_agr\n",
    "if_bid_sbmt_agr_index = total_data[total_data['if_bid_sbmt_agr'] == True].index\n",
    "total_data['bid_vol_sbmt_agr'] = 0\n",
    "total_data.loc[if_bid_sbmt_agr_index, 'bid_vol_sbmt_agr'] = total_data['size']\n",
    "\n",
    "if_bid_sbmt_index = total_data[total_data['type_direction'] == 1].index\n",
    "total_data['bid_vol_sbmt'] = 0\n",
    "total_data.loc[if_bid_sbmt_index, 'bid_vol_sbmt'] = total_data['size']\n",
    "\n",
    "total_data['lo_agr_bid'] = total_data['bid_vol_sbmt_agr'].rolling(50, min_periods = 1).sum()\\\n",
    "                         / total_data['bid_vol_sbmt'].rolling(50, min_periods = 1).sum()\n",
    "\n",
    "#-- new feature 10: effective spread\n",
    "'''\n",
    "The effective spread is computed as difference between the latest trade price and midprice \n",
    "                                    divided by midprice, then times 1000.\n",
    "This feature is derived from book: Irene Aldridge_High-frequency trading...(2013) P191\n",
    "Intuition: The effective spread measures how far, in percentage terms, the latest realized price \n",
    "           fell away from the simple mid price.\n",
    "'''\n",
    "if_lastest_trade_index = total_data[total_data['type'] == 4].index\n",
    "if_not_lastest_trade_index = total_data[total_data['type'] != 4].index\n",
    "total_data['lastest_trade_price'] = 0\n",
    "total_data.loc[if_lastest_trade_index,'lastest_trade_price'] = total_data['price']\n",
    "total_data.loc[if_not_lastest_trade_index,'lastest_trade_price'] = np.nan\n",
    "total_data['lastest_trade_price'].fillna(method='ffill',inplace = True)\n",
    "\n",
    "total_data['effective_spread'] = (total_data['lastest_trade_price'] / total_data['mid_price'] - 1) * 1000\n",
    "\n",
    "\n",
    "#-- export total_data\n",
    "new_features_resultpath = '/Users/macbook/Desktop/DLFall2018/codes/DL project/tensorflow-master'\n",
    "filename = new_features_resultpath + 'total_data.csv'\n",
    "total_data.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data.to_csv('total_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
