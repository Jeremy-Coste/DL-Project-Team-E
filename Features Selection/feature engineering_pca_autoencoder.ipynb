{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T12:44:09.131674Z",
     "start_time": "2018-10-02T12:44:09.129175Z"
    }
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:38:03.460211Z",
     "start_time": "2018-10-03T22:37:49.443514Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meihuaren/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import argparse\n",
    "# Needed for PCA\n",
    "from sklearn import decomposition\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderbook_cols = ['{}_{}_{}'.format(s,t,l) for l in range(1,6) for s in ['ask','bid'] for t in ['price', 'vol'] ]\n",
    "orderbook_ori = pd.read_csv('INTC_2012-06-21_34200000_57600000_orderbook_5.csv', \\\n",
    "                            header = None, names = orderbook_cols)\n",
    "orderbook1 = orderbook_ori.copy()\n",
    "orderbook1['mid_price'] = (orderbook1.iloc[:,0] + orderbook1.iloc[:,2]) / 2\n",
    "orderbook1['mid_price_mov'] = np.sign(orderbook1['mid_price'].shift(-1)-orderbook1['mid_price'])\n",
    "orderbook2 = orderbook1.dropna()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_all_array = scaler.fit_transform(orderbook2.iloc[:,:len(orderbook_cols)])\n",
    "orderbook = orderbook2.copy()\n",
    "orderbook.iloc[:,:len(orderbook_cols)] = x_all_array\n",
    "\n",
    "train_weight = 0.8\n",
    "cv_weight = 0.1\n",
    "split1 = int(orderbook.shape[0] * train_weight)\n",
    "split2 = int(orderbook.shape[0] * cv_weight)\n",
    "df_train = orderbook[:split1]\n",
    "df_cv = orderbook[split1:split1+split2]\n",
    "df_test = orderbook[split1+split2:]\n",
    "x_train = df_train.iloc[:,:len(orderbook_cols)]\n",
    "x_train_array = np.array(x_train)\n",
    "#y_train = df_train.iloc[:,-1]\n",
    "x_cv = df_cv.iloc[:,:len(orderbook_cols)]\n",
    "x_cv_array = np.array(x_cv)\n",
    "#y_cv = df_cv.iloc[:,-1]\n",
    "x_test = df_test.iloc[:,:len(orderbook_cols)]\n",
    "x_test_array = np.array(x_test)\n",
    "#y_test = df_test.iloc[:,-1]\n",
    "x_all = orderbook.iloc[:,:len(orderbook_cols)]\n",
    "x_all_array = np.array(x_all)\n",
    "#y_all = orderbook.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:38:03.476437Z",
     "start_time": "2018-10-03T22:38:03.462522Z"
    }
   },
   "outputs": [],
   "source": [
    "input_size = 20\n",
    "#re-constructed size\n",
    "output_size = 20\n",
    "\n",
    "# 3 hidden layers for encoder\n",
    "n_encoder_h_1 = 16\n",
    "n_encoder_h_2 = 8\n",
    "n_encoder_h_3 = 4\n",
    "\n",
    "# 3 hidden layers for decoder\n",
    "n_decoder_h_1 = 4\n",
    "n_decoder_h_2 = 8\n",
    "n_decoder_h_3 = 16\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 20 #200\n",
    "batch_size = 200\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:38:03.527109Z",
     "start_time": "2018-10-03T22:38:03.479053Z"
    }
   },
   "outputs": [],
   "source": [
    "def layer_batch_normalization(x, n_out, phase_train):\n",
    "    \"\"\"\n",
    "    Defines the network layers\n",
    "    input:\n",
    "        - x: input vector of the layer\n",
    "        - n_out: integer, depth of input maps - number of sample in the batch \n",
    "        - phase_train: boolean tf.Varialbe, true indicates training phase\n",
    "    output:\n",
    "        - batch-normalized maps   \n",
    "    \"\"\"\n",
    "\n",
    "    beta_init = tf.constant_initializer(value=0.0, dtype=tf.float32)\n",
    "    beta = tf.get_variable(\"beta\", [n_out], initializer=beta_init)\n",
    "    \n",
    "    gamma_init = tf.constant_initializer(value=1.0, dtype=tf.float32)\n",
    "    gamma = tf.get_variable(\"gamma\", [n_out], initializer=gamma_init)\n",
    "\n",
    "    #tf.nn.moment: https://www.tensorflow.org/api_docs/python/tf/nn/moments\n",
    "    #calculate mean and variance of x\n",
    "    batch_mean, batch_var = tf.nn.moments(x, [0], name='moments')\n",
    "\n",
    "    #tf.train.ExponentialMovingAverage:\n",
    "    #https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n",
    "    #Maintains moving averages of variables by employing an exponential decay.\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.9)\n",
    "    ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "    ema_mean, ema_var = ema.average(batch_mean), ema.average(batch_var)\n",
    "    \n",
    "    def mean_var_with_update():\n",
    "        with tf.control_dependencies([ema_apply_op]):\n",
    "            return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "       \n",
    "    #tf.cond: https://www.tensorflow.org/api_docs/python/tf/cond\n",
    "    #Return true_fn() if the predicate pred is true else false_fn()\n",
    "    mean, var = tf.cond(phase_train, mean_var_with_update, lambda: (ema_mean, ema_var))\n",
    "\n",
    "    reshaped_x = tf.reshape(x, [-1, 1, 1, n_out])\n",
    "    normed = tf.nn.batch_norm_with_global_normalization(reshaped_x, mean, var, beta, gamma, 1e-3, True)\n",
    "    #normed = tf.nn.batch_normalization(reshaped_x, mean, var, beta, gamma, 1e-3, True)\n",
    "    \n",
    "    return tf.reshape(normed, [-1, n_out])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:38:03.574224Z",
     "start_time": "2018-10-03T22:38:03.529227Z"
    }
   },
   "outputs": [],
   "source": [
    "def layer(x, weight_shape, bias_shape, phase_train):\n",
    "    \n",
    "    \"\"\"\n",
    "    Defines the network layers\n",
    "    input:\n",
    "        - x: input vector of the layer\n",
    "        - weight_shape: shape the the weight maxtrix\n",
    "        - bias_shape: shape of the bias vector\n",
    "        - phase_train: boolean tf.Varialbe, true indicates training phase\n",
    "    output:\n",
    "        - output vector of the layer after the matrix multiplication and non linear transformation\n",
    "    \"\"\"\n",
    "    \n",
    "    #initialize weights\n",
    "    weight_init = tf.random_normal_initializer(stddev=(1.0/weight_shape[0])**0.5)\n",
    "    W = tf.get_variable(\"W\", weight_shape, initializer=weight_init)\n",
    "    \n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    b = tf.get_variable(\"b\", bias_shape, initializer=bias_init)\n",
    "\n",
    "    logits = tf.matmul(x, W) + b\n",
    "    \n",
    "    #apply the non-linear function after the batch normalization\n",
    "    return tf.nn.sigmoid(layer_batch_normalization(logits, weight_shape[1], phase_train))\n",
    "    # Using sigmoid to avoid sharp transitions in neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T13:39:04.039484Z",
     "start_time": "2018-10-02T13:39:04.036698Z"
    }
   },
   "source": [
    "# Definition of the Encoder Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:38:03.598508Z",
     "start_time": "2018-10-03T22:38:03.576953Z"
    }
   },
   "outputs": [],
   "source": [
    "def encoder(x, n_code, phase_train):\n",
    "    \"\"\"\n",
    "    Defines the network encoder part\n",
    "    input:\n",
    "        - x: input vector of the encoder\n",
    "        - n_code: number of neurons in the code layer (output of the encoder - input of the decoder) \n",
    "        - phase_train: boolean tf.Varialbe, true indicates training phase\n",
    "    output:\n",
    "        - output vector: reduced dimension\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope(\"encoder\"):\n",
    "        \n",
    "        with tf.variable_scope(\"h_1\"):\n",
    "            h_1 = layer(x, [input_size, n_encoder_h_1], [n_encoder_h_1], phase_train)\n",
    "\n",
    "        with tf.variable_scope(\"h_2\"):\n",
    "            h_2 = layer(h_1, [n_encoder_h_1, n_encoder_h_2], [n_encoder_h_2], phase_train)\n",
    "\n",
    "        with tf.variable_scope(\"h_3\"):\n",
    "            h_3 = layer(h_2, [n_encoder_h_2, n_encoder_h_3], [n_encoder_h_3], phase_train)\n",
    "\n",
    "        with tf.variable_scope(\"code\"):\n",
    "            output = layer(h_3, [n_encoder_h_3, n_code], [n_code], phase_train)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T13:39:56.962874Z",
     "start_time": "2018-10-02T13:39:56.960040Z"
    }
   },
   "source": [
    "# Definition of the Decoder Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:38:03.634567Z",
     "start_time": "2018-10-03T22:38:03.600519Z"
    }
   },
   "outputs": [],
   "source": [
    "def decoder(x, n_code, phase_train):\n",
    "    \"\"\"\n",
    "    Defines the network encoder part\n",
    "    input:\n",
    "        - x: input vector of the decoder - reduced dimension vector\n",
    "        - n_code: number of neurons in the code layer (output of the encoder - input of the decoder)\n",
    "        - phase_train: boolean tf.Varialbe, true indicates training phase\n",
    "    output:\n",
    "        - output vector: reconstructed dimension of the initial vector\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope(\"decoder\"):\n",
    "        \n",
    "        with tf.variable_scope(\"h_1\"):\n",
    "            h_1 = layer(x, [n_code, n_decoder_h_1], [n_decoder_h_1], phase_train)\n",
    "\n",
    "        with tf.variable_scope(\"h_2\"):\n",
    "            h_2 = layer(h_1, [n_decoder_h_1, n_decoder_h_2], [n_decoder_h_2], phase_train)\n",
    "\n",
    "        with tf.variable_scope(\"h_3\"):\n",
    "            h_3 = layer(h_2, [n_decoder_h_2, n_decoder_h_3], [n_decoder_h_3], phase_train)\n",
    "\n",
    "        with tf.variable_scope(\"output\"):\n",
    "            output = layer(h_3, [n_decoder_h_3, output_size], [output_size], phase_train)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T13:40:11.932837Z",
     "start_time": "2018-10-02T13:40:11.929439Z"
    }
   },
   "source": [
    "# Definition of the Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:38:03.646821Z",
     "start_time": "2018-10-03T22:38:03.637606Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss is L2 measure\n",
    "def loss(output, x):\n",
    "    \"\"\"\n",
    "    Compute the loss of the auto-encoder\n",
    "    \n",
    "    intput:\n",
    "        - output: the output of the decoder\n",
    "        - x: true value of the sample batch - this is the input of the encoder\n",
    "        \n",
    "        the two have the same shape (batch_size * num_of_classes)\n",
    "    output:\n",
    "        - loss: loss of the corresponding batch (scalar tensor)\n",
    "    \n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"training\"):\n",
    "        \n",
    "        l2_measure = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(output, x)), 1))\n",
    "        train_loss = tf.reduce_mean(l2_measure)\n",
    "        train_summary_op = tf.summary.scalar(\"train_cost\", train_loss)\n",
    "        return train_loss, train_summary_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:38:03.661733Z",
     "start_time": "2018-10-03T22:38:03.650651Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using Adam as optimizer for training \t\n",
    "def training(cost, global_step):\n",
    "    \"\"\"\n",
    "    defines the necessary elements to train the network\n",
    "    \n",
    "    intput:\n",
    "        - cost: the cost is the loss of the corresponding batch\n",
    "        - global_step: number of batch seen so far, it is incremented by one \n",
    "        each time the .minimize() function is called\n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:38:03.705246Z",
     "start_time": "2018-10-03T22:38:03.664581Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(output, x):\n",
    "    \"\"\"\n",
    "    evaluates the accuracy on the validation set \n",
    "    input:\n",
    "        - output: prediction vector of the network for the validation set\n",
    "        - x: true value for the validation set\n",
    "    output:\n",
    "        - val_loss: loss of the autoencoder\n",
    "        - val_summary_op: summary of the loss\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope(\"validation\"):\n",
    "        \n",
    "        l2_norm = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(output, x, name=\"val_diff\")), 1))\n",
    "        \n",
    "        val_loss = tf.reduce_mean(l2_norm)\n",
    "        \n",
    "        val_summary_op = tf.summary.scalar(\"val_cost\", val_loss)\n",
    "        \n",
    "        return val_loss, val_summary_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T13:51:07.293398Z",
     "start_time": "2018-10-02T13:51:07.282274Z"
    }
   },
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:48:13.428734Z",
     "start_time": "2018-10-03T22:38:03.731572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 3.751135787\n",
      "Validation Loss: 5.238889\n",
      "Model saved in file: /Users/meihuaren/personal/DL_logs/ae/\n",
      "Epoch: 0002 cost = 3.508099950\n",
      "Validation Loss: 5.0794716\n",
      "Model saved in file: /Users/meihuaren/personal/DL_logs/ae/\n",
      "Epoch: 0003 cost = 3.395779326\n",
      "Validation Loss: 4.89628\n",
      "Model saved in file: /Users/meihuaren/personal/DL_logs/ae/\n",
      "Epoch: 0004 cost = 3.333917028\n",
      "Validation Loss: 4.719273\n",
      "Model saved in file: /Users/meihuaren/personal/DL_logs/ae/\n",
      "Epoch: 0005 cost = 3.278065537\n",
      "Validation Loss: 4.689906\n",
      "Model saved in file: /Users/meihuaren/personal/DL_logs/ae/\n",
      "Epoch: 0006 cost = 3.257912736\n",
      "Validation Loss: 4.7118177\n",
      "Model saved in file: /Users/meihuaren/personal/DL_logs/ae/\n",
      "Epoch: 0007 cost = 3.247385927\n",
      "Validation Loss: 4.638049\n",
      "Model saved in file: /Users/meihuaren/personal/DL_logs/ae/\n",
      "Epoch: 0008 cost = 3.202488335\n",
      "Validation Loss: 4.7283945\n",
      "Model saved in file: /Users/meihuaren/personal/DL_logs/ae/\n",
      "Epoch: 0009 cost = 3.180002682\n",
      "Validation Loss: 5.3466163\n",
      "Model saved in file: /Users/meihuaren/personal/DL_logs/ae/\n",
      "Epoch: 0010 cost = 3.148693366\n",
      "Validation Loss: 5.0681505\n",
      "Model saved in file: /Users/meihuaren/personal/DL_logs/ae/\n",
      "Epoch: 0011 cost = 3.182846285\n",
      "Validation Loss: 5.777073\n",
      "Model saved in file: /Users/meihuaren/personal/DL_logs/ae/\n",
      "Epoch: 0012 cost = 3.089987494\n",
      "Validation Loss: 4.6790476\n",
      "Model saved in file: /Users/meihuaren/personal/DL_logs/ae/\n",
      "Epoch: 0013 cost = 3.140315600\n",
      "Validation Loss: 4.755545\n",
      "Model saved in file: /Users/meihuaren/personal/DL_logs/ae/\n",
      "Epoch: 0014 cost = 3.093301770\n",
      "Validation Loss: 4.682992\n",
      "Model saved in file: /Users/meihuaren/personal/DL_logs/ae/\n",
      "Epoch: 0015 cost = 3.129524678\n",
      "Validation Loss: 5.8163724\n",
      "Model saved in file: /Users/meihuaren/personal/DL_logs/ae/\n",
      "Epoch: 0016 cost = 3.061247436\n",
      "Validation Loss: 4.8978524\n",
      "Model saved in file: /Users/meihuaren/personal/DL_logs/ae/\n",
      "Epoch: 0017 cost = 3.129303644\n",
      "Validation Loss: 6.744088\n",
      "Model saved in file: /Users/meihuaren/personal/DL_logs/ae/\n",
      "Epoch: 0018 cost = 3.060756288\n",
      "Validation Loss: 6.4510736\n",
      "Model saved in file: /Users/meihuaren/personal/DL_logs/ae/\n",
      "Epoch: 0019 cost = 3.118766181\n",
      "Validation Loss: 4.8783207\n",
      "Model saved in file: /Users/meihuaren/personal/DL_logs/ae/\n",
      "Epoch: 0020 cost = 3.092590550\n",
      "Validation Loss: 5.0096765\n",
      "Model saved in file: /Users/meihuaren/personal/DL_logs/ae/\n",
      "Optimization Done\n",
      "Test Loss: 7.7363663\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    #parser = argparse.ArgumentParser(description='Autoencoder')\n",
    "    #parser.add_argument('n_code', nargs=1, type=str)\n",
    "    #args = parser.parse_args(['--help'])\n",
    "    #n_code = args.n_code[0]\n",
    "    \n",
    "    #if a jupyter file, please comment the 4 above and use the one bellow\n",
    "    n_code = '2'\n",
    "    \n",
    "    #feel free to change with your own\n",
    "    model_path = '/Users/meihuaren/personal/DL_logs/ae/'\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        with tf.variable_scope(\"autoencoder_model\"):\n",
    "\n",
    "\n",
    "            #the input variables are first define as placeholder \n",
    "            # a placeholder is a variable/data which will be assigned later \n",
    "            x = tf.placeholder(\"float\", [None, 20]) # 20 original features\n",
    "            \n",
    "            phase_train = tf.placeholder(tf.bool)\n",
    "\n",
    "            #define the encoder \n",
    "            code = encoder(x, int(n_code), phase_train)\n",
    "\n",
    "            #define the decoder\n",
    "            output = decoder(code, int(n_code), phase_train)\n",
    "\n",
    "            #compute the loss \n",
    "            cost, train_summary_op = loss(output, x)\n",
    "\n",
    "            #initialize the value of the global_step variable \n",
    "            # recall: it is incremented by one each time the .minimise() is called\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "            train_op = training(cost, global_step)\n",
    "\n",
    "            #evaluate the accuracy of the network (done on a validation set)\n",
    "            eval_op, val_summary_op = evaluate(output, x)\n",
    "\n",
    "            summary_op = tf.summary.merge_all()\n",
    "\n",
    "            #save and restore variables to and from checkpoints.\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            #defines a session\n",
    "            sess = tf.Session()\n",
    "\n",
    "            # summary writer\n",
    "            #https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter\n",
    "            train_writer = tf.summary.FileWriter(model_path, graph=sess.graph)\n",
    "            val_writer   = tf.summary.FileWriter(model_path, graph=sess.graph)\n",
    "\n",
    "            #initialization of the variables\n",
    "            init_op = tf.global_variables_initializer()\n",
    "\n",
    "            sess.run(init_op)\n",
    "\n",
    "            # Training cycle\n",
    "            for epoch in range(training_epochs):\n",
    "\n",
    "                avg_cost = 0.\n",
    "                total_batch = int(x_train_array.shape[0]/batch_size)\n",
    "                \n",
    "                #train_writer = tf.summary.FileWriter(model_path+str(epoch)+'/model.ckpt', graph=sess.graph)\n",
    "                #val_writer   = tf.summary.FileWriter(model_path+str(epoch)+'/model.ckpt', graph=sess.graph)\n",
    "                \n",
    "                # Loop over all batches\n",
    "                for i in range(total_batch):\n",
    "                    \n",
    "                    minibatch_x = x_train_array[i*batch_size:(i+1)*batch_size]\n",
    "                    \n",
    "                    # Fit training using batch data\n",
    "                    #the training is done using the training dataset\n",
    "                    _, new_cost, train_summary = sess.run([train_op, cost, train_summary_op], feed_dict={x: minibatch_x, phase_train: True})\n",
    "                    \n",
    "                    train_writer.add_summary(train_summary, sess.run(global_step))\n",
    "                    \n",
    "                    # Compute average loss\n",
    "                    avg_cost += new_cost/total_batch\n",
    "                \n",
    "                # Display logs per epoch step\n",
    "                if epoch % display_step == 0:\n",
    "                    \n",
    "                    print(\"Epoch:\", '%04d' % (epoch+1), \"cost =\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "                    train_writer.add_summary(train_summary, sess.run(global_step))\n",
    "\n",
    "                    validation_loss, val_summary = sess.run([eval_op, val_summary_op], feed_dict={x: x_cv_array, phase_train: False})\n",
    "                    \n",
    "                    val_writer.add_summary(val_summary, sess.run(global_step))\n",
    "                    \n",
    "                    print(\"Validation Loss:\", validation_loss)\n",
    "\n",
    "                    save_path = saver.save(sess, model_path)\n",
    "                    print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "\n",
    "            print(\"Optimization Done\")\n",
    "\n",
    "            test_loss = sess.run(eval_op, feed_dict={x: x_test_array, phase_train: False})\n",
    "\n",
    "            print(\"Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing PCA\n",
      "PCA Codes\n",
      "[[  6.83314187  -2.69809518]\n",
      " [  6.83204746  -2.69698052]\n",
      " [  6.81168241  -2.70884073]\n",
      " ...\n",
      " [-15.09461315  15.5592874 ]\n",
      " [-15.07970768  15.53025868]\n",
      " [-15.08080209  15.53137334]]\n",
      "\n",
      "\n",
      "Starting Autoencoder /Users/meihuaren/personal/DL_logs/ae/\n",
      "\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /Users/meihuaren/personal/DL_logs/ae/\n",
      "Model restored from file: None\n",
      "Running Autoencoder & Autoencoder Codes\n",
      "\n",
      "\n",
      "[[0.06358596 0.3159923 ]\n",
      " [0.06355276 0.31604666]\n",
      " [0.06350567 0.316114  ]\n",
      " ...\n",
      " [0.39423656 0.15631337]\n",
      " [0.39152122 0.1568615 ]\n",
      " [0.39070725 0.15702535]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    #feel free to change with your own\n",
    "    args_savepath = '/Users/meihuaren/personal/DL_logs/ae/'\n",
    "    new_features_resultpath = '/Users/meihuaren/personal/OR_2018fall/Courses/E4720 Deep Learning/project_coding/Team E_code/'\n",
    "    n_code = 2\n",
    "    \n",
    "    #=====================================\n",
    "    # PCA\n",
    "    print ('Performing PCA')\n",
    "    pca = decomposition.PCA(n_components=2) # grid search for the parameter\n",
    "    pca.fit(x_train_array) # use train data for feature selection in order to avoid look ahead bias\n",
    "    print('PCA Codes')\n",
    "    pca_codes = pca.transform(x_all_array)\n",
    "    print(pca_codes)\n",
    "    pca_codes_df = pd.DataFrame(pca_codes)\n",
    "    ob_new_pca = pd.concat([pca_codes_df,orderbook.iloc[:,-1]],axis = 1)\n",
    "    filename = new_features_resultpath + 'ob_new_pca.csv'\n",
    "    ob_new_pca.to_csv(filename)\n",
    "    \n",
    "    '''\n",
    "    print('Re-Constructing')\n",
    "    # transform data into its original space\n",
    "    pca_reconstructed = pca.inverse_transform(pca_codes[:20])\n",
    "    #print(pca_reconstructed)\n",
    "    '''\n",
    "\n",
    "    #=====================================\n",
    "    # AutoEncoder\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        with tf.variable_scope(\"autoencoder_model\"):\n",
    "\n",
    "            x = tf.placeholder(\"float\", [None, 20]) # 20 original features\n",
    "            \n",
    "            phase_train = tf.placeholder(tf.bool)\n",
    "\n",
    "            code = encoder(x, n_code, phase_train)\n",
    "\n",
    "            output = decoder(code, n_code, phase_train)\n",
    "\n",
    "            cost, train_summary_op = loss(output, x)\n",
    "\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "            train_op = training(cost, global_step)\n",
    "\n",
    "            eval_op, val_summary_op = evaluate(output, x)\n",
    "\n",
    "            #saver = tf.train.Saver()\n",
    "            #sess = tf.Session()\n",
    "            print('\\n')\n",
    "            print('Starting Autoencoder', args_savepath ) #args.savepath[0]\n",
    "            print('\\n')\n",
    "            \n",
    "            sess = tf.Session()\n",
    "            saver = tf.train.Saver()\n",
    "            save_path = saver.restore(sess, args_savepath ) #args.savepath[0]\n",
    "            print(\"Model restored from file: %s\" % save_path)\n",
    "\n",
    "            print('Running Autoencoder & Autoencoder Codes')\n",
    "            print('\\n')\n",
    "            \n",
    "            ae_codes = sess.run(code, feed_dict={x: x_all_array, phase_train: True})\n",
    "            print(ae_codes)\n",
    "            ae_codes_df = pd.DataFrame(ae_codes)\n",
    "            ob_new_ae = pd.concat([ae_codes_df,orderbook.iloc[:,-1]],axis = 1)\n",
    "            filename = new_features_resultpath + 'ob_new_ae.csv'\n",
    "            ob_new_ae.to_csv(filename)\n",
    "            \n",
    "            #ae_codes, ae_reconstruction = sess.run([code, output], feed_dict={x: mnist.test.images*np.random.randint(2, size=(784)), phase_train: True})\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
